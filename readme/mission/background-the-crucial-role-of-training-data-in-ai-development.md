# Background: The Crucial Role of Training Data in AI Development

According to James Betker, a researcher at OpenAI, the success and complexity of AI systems are increasingly determined by the quality and diversity of their training data rather than their model design, architecture, or other features. His extensive experience in training generative models has led him to a critical realization:

> "Model behavior is not determined by architecture, hyperparameters, or optimizer choices. It’s determined by your dataset, nothing else."

Betker's insights underscore that, regardless of the model type—whether diffusion conv-unets or ViT generators—if they are trained on the same dataset, they will eventually produce similar results. This convergence highlights the fact that the dataset's richness and variety are the primary factors influencing the model's capabilities.

This revelation has significant implications for the AI industry. It shifts the focus from optimizing model architectures to curating and expanding datasets. As Betker notes, "Sufficiently large diffusion conv-unets produce the same images as ViT generators. AR sampling produces the same images as diffusion." This uniformity in outcomes across different model types suggests that the dataset's content is the true driver of model performance.

For AI developers and companies, this means that investing in high-quality, diverse datasets is paramount. It also implies that when discussing different AI models like "Lambda," "ChatGPT," "Bard," or "Claude," the real differentiator is not the model weights or configurations but the underlying dataset each model was trained on.

Meanwhile, large corporations' annual spending on data procurement is rapidly increasing from tens of billions to an estimated $300 to $500 billion, or even higher. Scale AI, a leading data provider in the AI industry, recently raised $1 billion with a valuation exceeding $14 billion and is expected to go public soon. In stark contrast to the success and admiration of leading AI companies, data providers like Scale AI often employ teams or subcontractors in developing countries, where workers are hired at extremely low costs and subjected to long hours without any benefits or job security.

<figure><img src="../../.gitbook/assets/image7 (2).png" alt=""><figcaption></figcaption></figure>

In the context of Pundi AIFX, this emphasis on data is particularly relevant. The platform's mission to democratize data contribution and ensure fair compensation for data providers aligns with the understanding that data quality is the cornerstone of AI innovation. By facilitating the contribution of diverse, high-quality datasets, Pundi AIFX can play a pivotal role in advancing AI technology, ensuring that models trained on its platform are as powerful and nuanced as possible.

The AI data marketplace, as envisioned by Pundi AIFX, becomes even more critical when we consider Betker's insights. It highlights the need for decentralized, transparent, and incentivized data ecosystems where contributors are rewarded for the invaluable data they provide, driving the next wave of AI advancements.

Here are some statistics for reference:

> OpenAI has spent hundreds of millions of dollars licensing content from news publishers, stock media libraries, and more to train its AI models — a budget far beyond that of most academic research groups, nonprofits, and startups.
>
> Meta even considered acquiring publisher Simon & Schuster for the rights to e-book excerpts (ultimately, Simon & Schuster sold to private equity firm KKR for $1.62 billion in 2023).
>
> Stock media library Shutterstock has signed deals with AI vendors ranging from $25 million to $50 million.
>
> Reddit claims to have made hundreds of millions from licensing data to organizations such as Google and OpenAI.
